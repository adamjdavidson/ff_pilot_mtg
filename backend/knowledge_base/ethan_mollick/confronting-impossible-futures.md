# Confronting Impossible Futures

Source: [One Useful Thing Newsletter](https://www.oneusefulthing.org/p/confronting-impossible-futures)
Date: July 22, 2024
Tags: AI, future planning, AGI, uncertainty, scenario planning, strategy

We shouldn't be certain about what is next, but we should plan for it

I've been talking to a number of organizations – companies, academic institutions, non-profits, government agencies – about AI, and I've been struck by something. Almost all of these groups have long-term planning processes. Many are planning for 5, 10, even 20 years in the future. Yet, as far as I can tell, very few are including the potential for continued massive improvements in AI in their planning. One of the primary reasons for this appears to be extreme uncertainty: we just don't know what will happen next, and there is expert disagreement about the future path of AI. I think that justifies caution in prediction, but not ignoring the issue altogether. Here is my view, as a non-expert, of what organizations should be thinking about. (I wrote about this issue in Harvard Business Review, if you are looking for a more formal treatment).

## The future of AI: continued growth or a ceiling?

As we think about the future of AI, there is a massive bifurcation among experts. Some experts think that AI capabilities are likely to continue to grow, potentially exponentially. Other experts think that Large Language Models, including current frontier models like GPT-4 and Claude Opus, have largely seen the end of their growth in capabilities, and will mostly get better in incremental ways.

There continues to be a lively ongoing academic debate among experts, and the fact we are seeing such disagreements means that non-experts (like me, and if you are not a researcher building AI systems, probably like you), have even less ability to predict with any certainty what will happen next. And, as we will see, even the experts have a poor track record in their predictions.

To frame this discussion, I want to talk a little about AGI, a term that many are uncomfortable with, but that I think is a useful shorthand. Many inside AI labs believe it may be possible to develop Artificial General Intelligence (AGI) in the near future. For example, Sam Altman has been very direct about what OpenAI is optimizing for: "The benefits of getting to superintelligence and not messing it up, not having it kill us all…would be so extreme that the good case is really, really good. And that's why we're doing it." Other AI labs have similar goals: Meta states that "we still remain focused on our north star: artificial general intelligence." For many AI labs, the goal is AGI.

What is AGI? One definition, from anthropic, is "a general purpose AI that's capable of human-level performance on a wide range of tasks." Many people appear to believe that AGI, at a minimum, would represent a massive economic disruption in the world, well beyond what we've experienced with past industrial revolutions – and, for better or worse, many (though not all) also believe it could be a transformative technology. 

How likely is AGI? Many of the most famous AI researchers seem to believe AGI is coming, perhaps soon.  A recent informal survey of 37 experts (including many insiders in AI labs, but also some who are known AI skeptics) from the Future of Life Institute found that a majority of respondents suggested that AGI could be achievable by 2027-2033. By 2028, 20% of the experts selected suggested a 50% chance of AGI. By 2033, 50% of respondents thought there was a 50% or higher chance of AGI.

But these views aren't universal. Many academics, and even lots of insiders at AI labs, would disagree that AGI is coming soon. Empirical work on AI predictions has found that experts are just as bad at predicting AI as regular people, experts (and the rest of us) tend to predict that AI will achieve human-level performance in 15-25 years from the time they are asked the question, and, to the extent we can check past predictions, they tend to be overconfident and wrong, either too early or too late. In this view of the future, Current models have reached, or are approaching, some fundamental limits in their capability, and are unlikely to improve rapidly.

## Why isn't this being taken more seriously?

Imagine, for a moment, that the majority of AI insiders in that survey mentioned above are correct in their estimates, and that we see AGI by 2033. We would likely see a transformation in jobs, learning, and work – and, for better and worse, likely our entire society. Yet organizations are not doing much to prepare for this. I see several reasons for this.

First, discussions about AGI are often in terms of superintelligence – an AI system that dramatically exceeds human capabilities, potentially by an unbounded amount – which probably feels unplannable. Debating superintelligence also leads to a division between the superintelligence believers (a group labeled by others as "doomers") and the superintelligence skeptics. All of this tends to lead people to focus on extreme scenarios, rather than near-term practical considerations.

Second, some of the future-looking conversation around AGI involves issues like AI rights, the implications of digital minds (if they can be created), and a host of other issues that are fascinating but that may seem distracting for practical planning today. While these issues may become very real if AGI is achieved, the conversation around them can appear to support the common feeling that all of this is just science fiction.

But whether fully-fledged AGI is achieved in the timeframes suggested, current AI progress is likely to continue, meaning we will continue to see a disruption of human capability. Today, many jobs are already being partially or fully automated, and AI is being used in almost all major companies by individuals to boost productivity. The disruption is already happening, and it is worth planning for a future where it increases.

So, I think we need to recognize that progress in AI capabilities is likely to continue, and it has the potential to transform the world – and many organizations still need an approach to planning for a radically-uncertain AI future. Let me offer a few thoughts on how to plan under such uncertainty.

## Planning for the future

First, as much as I think AI is going to be transformational, my view is we should all maintain high uncertainty in our predictions. We just don't know what is going to happen, and given the many divergent opinions among real AI experts, plus the track record of previous predictions, any definitive statement about the future of AI is likely to be wrong.

Second, while maintaining uncertainty, I think we should still develop plans for multiple potential scenarios, including ones in which AI progress continues to accelerate. This means considering both the more mundane scenarios of AI progress in the near-term (more jobs partially or fully automated, more people using AI tools) and what could happen if AI dramatically advances.

Third, jobs, learning, and work are being disrupted right now by AI, and whatever happens next, it seems unlikely that the importance of these systems will decline. Better to be prepared than not, though we should still expect to be surprised.

I want to suggest a specific tool for organizations to use in dealing with this: scenario planning. Scenario planning is a structured approach to thinking about the future, and it is particularly valuable when the potential outcomes are highly uncertain. It involves developing plausible, but different, scenarios for the future, and then considering what these would mean for a company – both in terms of threats, but also opportunities. The act of planning this way can help a company in two ways: directly, as they prepare for multiple plausible futures, but also managerially, as the process of planning scenarios together builds a common understanding and helps break through some organizational barriers.

It is almost certainly a mistake to plan on any given AI future, but we should be considering all of them. We should also be evaluating our actions today, including our preparations for AI, against what we want to see in the future. What kind of future are we building? What are we doing to get there? And, in a world of such uncertainty, what steps can we take now to prepare for a range of potential outcomes, both good and bad?

So that's my view: we should be uncertain about the future, especially the future of AI, but that doesn't mean we should ignore the potential for continued dramatic progress in these systems. Instead, we should widen the range of future scenarios we consider, and make sure we're prepared for a range of futures rather than just a single predicted outcome.